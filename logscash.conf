input {
  beats {
    port => 5044
    client_inactivity_timeout => 300
    ssl => "{{ logstash_beats_ssl_enabled }}"
    ssl_certificate => "/etc/logstash/certs/logstash.crt"
    ssl_key => "/etc/logstash/certs/logstash.key"
    ssl_certificate_authorities => ["/etc/logstash/certs/ca.crt"]
  }
}

filter {
  if [fields][log_type] == "minecraft" {
    grok {
      patterns_dir => ["/etc/logstash/patterns"] # Path to custom patterns directory
      match => { "message" => "%{TIMESTAMP_ISO8601:log_timestamp} \[%{DATA:thread_name}\/%{LOGLEVEL:log_level}\] \[%{MINECRAFT_LOG_SOURCE:logger}\]: %{GREEDYDATA:log_message}" }
      tag_on_failure => ["_grokparsefailure_minecraft"]
    }
    date {
      match => [ "log_timestamp", "ISO8601" ]
      target => "@timestamp"
    }
    # Extract more details using additional filters like dissect or kv
    # Example using the dissect filter (adjust the pattern as needed)
    dissect {
        mapping => {
            "log_message" => "%{?player_name} %{action} %{?item_name} - %{?world} - X:%{?x}, Y:%{?y}, Z:%{?z}"
        }
    }
  } else if [fields][log_type] == "haproxy" {
      grok {
        match => {
          "message" => [
            "%{IPORHOST:client_ip}:%{POSINT:client_port} \[%{HTTPDATE:accept_date}\] %{NOTSPACE:frontend_name} %{NOTSPACE:backend_name}/%{NOTSPACE:server_name} %{NUMBER:time_request}/%{NUMBER:time_queue}/%{NUMBER:time_backend_connect}/%{NUMBER:time_backend_response}/%{NUMBER:time_duration} %{NUMBER:http_status_code} %{NUMBER:bytes_read} %{DATA:captured_request_cookie} %{DATA:captured_response_cookie} %{NOTSPACE:termination_state} %{NUMBER:actconn}/%{NUMBER:feconn}/%{NUMBER:beconn}/%{NUMBER:srv_conn}/%{NUMBER:retries} %{NUMBER:srv_queue}/%{NUMBER:backend_queue} \"%{WORD:http_verb} %{URIPATHPARAM:request_path} HTTP/%{NUMBER:http_version}\"",
            "%{IPORHOST:client_ip}:%{POSINT:client_port} \[%{HTTPDATE:accept_date}\] %{NOTSPACE:frontend_name} %{NOTSPACE:backend_name}/%{NOTSPACE:server_name} %{NUMBER:time_request}/%{NUMBER:time_queue}/%{NUMBER:time_backend_connect}/%{NUMBER:time_backend_response}/%{NUMBER:time_duration} %{NUMBER:http_status_code} %{NUMBER:bytes_read} %{DATA:captured_request_cookie} %{DATA:captured_response_cookie} %{NOTSPACE:termination_state} %{NUMBER:actconn}/%{NUMBER:feconn}/%{NUMBER:beconn}/%{NUMBER:srv_conn}/%{NUMBER:retries} %{NUMBER:srv_queue}/%{NUMBER:backend_queue} \"-\""
          ]
        }
        tag_on_failure => ["_grokparsefailure_haproxy"]
      }
      date {
        match => [ "accept_date", "dd/MMM/yyyy:HH:mm:ss Z" ]
        target => "@timestamp"
      }
    } else if [fields][log_type] == "syslog" {
        grok {
            patterns_dir => ["/etc/logstash/patterns"]
            match => { "message" => "%{SYSLOG_TIMESTAMP:timestamp} %{SYSLOGHOST:hostname} %{DATA:program}(?:\[%{POSINT:pid}\])?: %{GREEDYDATA:message}" }
            tag_on_failure => ["_grokparsefailure_syslog"]
        }
        date {
            match => [ "timestamp", "MMM  d HH:mm:ss", "MMM dd HH:mm:ss", "ISO8601" ]
            target => "@timestamp"
        }
        # Add fingerprinting or other identifiers for better tracking
        fingerprint {
            source => ["hostname", "program"]
            target => "[@metadata][fingerprint]"
        }
    }

    if "_grokparsefailure" in [tags] {
        mutate {
            add_field => { "[grok][failure_reason]" => "Grok parsing failed for one or more patterns." }
        }
    }

  geoip {
    source => "client_ip"
    target => "geoip"
  }

  dns {
    reverse => ["client_ip"]
    action => "replace"
  }
  useragent {
    source => "message"
    target => "user_agent"
  }
  # Further enrich with other filters based on extracted data (e.g., kv, xml, etc.)
  if [log_type] == "minecraft" and "world" in [log_message] {
    kv {
        source => "log_message"
        field_split => "&"
        value_split => "="
        target => "url_params"
    }
  }

  mutate {
    add_field => {
      "[@metadata][index]" => "%{[fields][log_type]}-%{+YYYY.MM.dd}"
    }
    # Convert fields to specific data types if needed
    convert => {
      "time_duration" => "float"
      "bytes_read" => "integer"
    }
  }
}

output {
  elasticsearch {
    hosts => [
      {% for host in groups['elasticsearch_servers'] -%}
        "https://{{ hostvars[host].ansible_host }}:{{ es_http_port }}"
        {% if not loop.last %},{% endif %}
      {%- endfor %}
    ]
    index => "%{[@metadata][index]}"
    ssl => true
    cacert => '/etc/logstash/certs/ca.crt'
    user => "logstash_internal"
    password => "{{ logstash_password }}"
  }
}
